{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading and cleaning data",
   "id": "fc221bf7ea6050bf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:29.127466Z",
     "start_time": "2025-01-21T15:57:28.680920Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:29.799534Z",
     "start_time": "2025-01-21T15:57:29.136674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset source: https://huggingface.co/datasets/ronaldahmed/scitechnews\n",
    "df_train = pd.read_json('data/train.json', lines=True)\n",
    "df_train.head()"
   ],
   "id": "8ec881a4dc6ad8c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id                                           pr-title  \\\n",
       "0   0  New York City's Vaccine Passport Plan Renews O...   \n",
       "1   1  Facebook Disables Accounts Tied to NYU Researc...   \n",
       "2   2  Teenage Girls in Northern Nigeria 'Open Their ...   \n",
       "3   3  3D 'Heat Map' Animation Shows How Seizures Spr...   \n",
       "4   4  Endlessly Changing Playground Teaches AIs to M...   \n",
       "\n",
       "                                          pr-article  \\\n",
       "0  When New York City announced on Tuesday that i...   \n",
       "1  Facebook Inc. has disabled the personal accoun...   \n",
       "2  KANO, Nigeria, Aug 2 (Reuters) - Teenage girls...   \n",
       "3  For 29 years, from the time she was 12, Rashet...   \n",
       "4  What did they learn? Some of DeepMind's XLand ...   \n",
       "\n",
       "                                          pr-summary sc-title sc-abstract  \\\n",
       "0  New York's City's mandate that people must sho...                        \n",
       "1  Facebook has disabled the personal accounts of...                        \n",
       "2  The Kabara non-governmental organization (NGO)...                        \n",
       "3  University of California, San Francisco (UCSF)...                        \n",
       "4  Alphabet's DeepMind Technologies has developed...                        \n",
       "\n",
       "  sc-section_names sc-sections sc-article sc-authors  \n",
       "0               []          []                    []  \n",
       "1               []          []                    []  \n",
       "2               []          []                    []  \n",
       "3               []          []                    []  \n",
       "4               []          []                    []  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pr-title</th>\n",
       "      <th>pr-article</th>\n",
       "      <th>pr-summary</th>\n",
       "      <th>sc-title</th>\n",
       "      <th>sc-abstract</th>\n",
       "      <th>sc-section_names</th>\n",
       "      <th>sc-sections</th>\n",
       "      <th>sc-article</th>\n",
       "      <th>sc-authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>New York City's Vaccine Passport Plan Renews O...</td>\n",
       "      <td>When New York City announced on Tuesday that i...</td>\n",
       "      <td>New York's City's mandate that people must sho...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Facebook Disables Accounts Tied to NYU Researc...</td>\n",
       "      <td>Facebook Inc. has disabled the personal accoun...</td>\n",
       "      <td>Facebook has disabled the personal accounts of...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Teenage Girls in Northern Nigeria 'Open Their ...</td>\n",
       "      <td>KANO, Nigeria, Aug 2 (Reuters) - Teenage girls...</td>\n",
       "      <td>The Kabara non-governmental organization (NGO)...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3D 'Heat Map' Animation Shows How Seizures Spr...</td>\n",
       "      <td>For 29 years, from the time she was 12, Rashet...</td>\n",
       "      <td>University of California, San Francisco (UCSF)...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Endlessly Changing Playground Teaches AIs to M...</td>\n",
       "      <td>What did they learn? Some of DeepMind's XLand ...</td>\n",
       "      <td>Alphabet's DeepMind Technologies has developed...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now keep only pertinent columns:",
   "id": "fd18419a66cb31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:29.977787Z",
     "start_time": "2025-01-21T15:57:29.964792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = df_train[['pr-title', 'pr-article']]\n",
    "\n",
    "df_train['pr-article'] = df_train['pr-article'].astype('string')\n",
    "df_train['pr-title'] = df_train['pr-title'].astype('string')\n",
    "\n",
    "df_train.head()"
   ],
   "id": "f0cc4555a283f467",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            pr-title  \\\n",
       "0  New York City's Vaccine Passport Plan Renews O...   \n",
       "1  Facebook Disables Accounts Tied to NYU Researc...   \n",
       "2  Teenage Girls in Northern Nigeria 'Open Their ...   \n",
       "3  3D 'Heat Map' Animation Shows How Seizures Spr...   \n",
       "4  Endlessly Changing Playground Teaches AIs to M...   \n",
       "\n",
       "                                          pr-article  \n",
       "0  When New York City announced on Tuesday that i...  \n",
       "1  Facebook Inc. has disabled the personal accoun...  \n",
       "2  KANO, Nigeria, Aug 2 (Reuters) - Teenage girls...  \n",
       "3  For 29 years, from the time she was 12, Rashet...  \n",
       "4  What did they learn? Some of DeepMind's XLand ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr-title</th>\n",
       "      <th>pr-article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York City's Vaccine Passport Plan Renews O...</td>\n",
       "      <td>When New York City announced on Tuesday that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Facebook Disables Accounts Tied to NYU Researc...</td>\n",
       "      <td>Facebook Inc. has disabled the personal accoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Teenage Girls in Northern Nigeria 'Open Their ...</td>\n",
       "      <td>KANO, Nigeria, Aug 2 (Reuters) - Teenage girls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3D 'Heat Map' Animation Shows How Seizures Spr...</td>\n",
       "      <td>For 29 years, from the time she was 12, Rashet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Endlessly Changing Playground Teaches AIs to M...</td>\n",
       "      <td>What did they learn? Some of DeepMind's XLand ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:30.094631Z",
     "start_time": "2025-01-21T15:57:30.051554Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.info()",
   "id": "cf7c0b9d942124b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26638 entries, 0 to 26637\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   pr-title    26638 non-null  string\n",
      " 1   pr-article  26638 non-null  string\n",
      "dtypes: string(2)\n",
      "memory usage: 416.3 KB\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The dataset has empty article rows, which are not important for our task.\n",
    "\n",
    "Remove empty articles (articles where title is all the info., so they don't need a new title)"
   ],
   "id": "97ecde408eaef660"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:30.289402Z",
     "start_time": "2025-01-21T15:57:30.276106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = df_train[df_train['pr-article'] != '']\n",
    "print('Train dataframe size without blanks: ', len(df_train))"
   ],
   "id": "7250fd432b47632c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe size without blanks:  12735\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print a random article and title:",
   "id": "c12192393ff2e987"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:30.504494Z",
     "start_time": "2025-01-21T15:57:30.408133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random_article_idx = random.randint(0, len(df_train))\n",
    "row = df_train.iloc[random_article_idx]\n",
    "print('News example: ', random_article_idx)\n",
    "print('Title:    \\n', row['pr-title'])\n",
    "print('Article:  \\n', row['pr-article'])"
   ],
   "id": "6c60383bcbdf922f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News example:  7727\n",
      "Title:    \n",
      " TC Study Finds MOOC Reality Not Yet Meeting High Expectations\n",
      "Article:  \n",
      " The rollout of MOOCs, or massive open online courses, three years ago by some of the country's leading universities triggered predictions that bricks-and-mortar campuses would soon be obsolete and that learning would be forever changed.\n",
      "Well, maybe - but not any time soon, according to one of the first comprehensive studies of MOOCs from the perspective of institutions, just released by researchers at Teachers College. The study is based on 83 interviews with faculty members, administrators, researchers, and other actors in the MOOCspace from 62 institutions, mostly in the U.S. It includes 13 case studies to illustrate how MOOCs are successfully being used to address institutional goals.\n",
      "The study finds that a primary goal for institutions offering MOOCs is to extend institutional reach and access to education. \"MOOCs are providing educational opportunities to millions of individuals across the world,\" write Fiona M. Hollands (Ph.D. '03) and Devayani Tirthali (Ed.D. '13, Ed.M. '12), respectively of the College's Center for Benefit-Cost Studies of Education and Institute for Learning Technologies, in their report, \"MOOCs: Expectations and Reality.\" However, to date, \"most MOOC participants are already well-educated and employed.\" Consequently, \"the evidence suggests that MOOCs currently are falling far short of 'democratizing' education and may, for now, be doing more to increase gaps in access to education than to diminish them.\"\n",
      "Using MOOCs to \"build and maintain brand\" is another frequently mentioned institutional goal, but while MOOCs often generate media attention, \"isolating and measuring impact of any new initiative on brand is a difficult exercise,\" the report suggests. Indeed, increasing access to online offerings and enhancing brand may be contradictory goals, because the former can be seen as diminishing the selectiveness of the offering institution.\n",
      "Hollands and Tirthali report that it is still too early to know whether MOOCs can live up to the hype of providing a cost-effective means for producing better educational outcomes on a mass scale. Cost analyses of MOOC production and delivery at four different institutions found that costs ranged from $39,000 to $325,000 per MOOC. \"MOOCs have, so far, proved to be a significant drain on time and money for institutions,\" Hollands and Tirthali write. That picture could change as institutions reuse MOOC materials, share them with each other, develop common courses, replace on-campus courses with MOOCs, and save on faculty teaching time and facilities costs. Revenue streams from MOOCs have been slow in materializing. Unless costs of MOOC production can be recovered through fees, Hollands and Tirthali speculate that \"free, non-credit bearing MOOCs are likely to remain available only from the wealthiest institutions that can subsidize the costs from other sources of funds.\"\n",
      "As for improving learning outcomes, MOOCs, on the whole, cannot yet make that claim. \"While interviewees provided many examples of how MOOCs have been used to change instruction, for the most part, actual impact on educational outcomes has not been documented in any rigorous fashion,\" the report asserts. However, two cases highlighted in the report provide examples of positive effects on student performance as a result of adopting MOOC-inspired strategies such as frequent assessment and automatic feedback, or of integrating MOOCs into flipped on-campus courses. Hollands and Tirthali conclude that \"while the potential for MOOCs to contribute significantly to the development of personalized and adaptive learning is high, the reality is far from being achieved.\" To get there, \"a great deal of coordination and collaboration among content experts, instructors, researchers, instructional designers, and programmers will be necessary.\"\n",
      "Hollands and Tirthali make several recommendations to institutions for increasing the value of MOOCs to improve access and educational outcomes, and to reduce the costs of higher education. These include:\n",
      "Hollands is the Associate Director at the Center for Benefit-Cost Studies of Education at Teachers College, Columbia University. Devayani Tirthali is a Research Associate at TC's Institute for Learning Technologies.\n",
      "The Center for Benefit-Cost Studies of Education (CBCSE) seeks to improve the efficiency with which public and private resources are employed in education. The center conducts research to determine the costs of educational programs as well as the economic value of program impacts in order to encourage educators, evaluators, and policymakers to consider these factors in conjunction with program effectiveness in addressing educational goals. CBCSE is co-directed by Henry M. Levin, TC's William Heard Kilpatrick Professor of Economics and Education, and Clive Belfield, Associate Professor of Economics at Queens College, City University of New York.\n",
      "Published Thursday, May. 15, 2014\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See article lengths distribution (in chars and words):",
   "id": "32561a6dcab5a38c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:32.380449Z",
     "start_time": "2025-01-21T15:57:30.602468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "article_lengths = df_train['pr-article'].str.len()\n",
    "article_word_counts = df_train['pr-article'].str.split().str.len()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].hist(article_lengths, bins=150, color='orange')\n",
    "ax[0].set_xlim(0, 15000)\n",
    "ax[0].set_xlabel('Article length (characters)')\n",
    "ax[0].set_ylabel('Counts')\n",
    "\n",
    "ax[1].hist(article_word_counts, bins=150, color='blue')\n",
    "ax[1].set_xlim(0, 3000)\n",
    "ax[1].set_xlabel('Article length (words)')\n",
    "ax[1].set_ylabel('Counts')"
   ],
   "id": "22f6642e855b0e05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Counts')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Repeat cleaning process for valid and test datasets",
   "id": "8ea533082d43f725"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:33.621720Z",
     "start_time": "2025-01-21T15:57:32.401136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_valid = pd.read_json('data/valid.json', lines=True)\n",
    "df_valid = df_valid[['pr-title', 'pr-article']]\n",
    "df_valid['pr-article'] = df_valid['pr-article'].astype('string')\n",
    "df_valid['pr-title'] = df_valid['pr-title'].astype('string')\n",
    "\n",
    "df_test = pd.read_json('data/test.json', lines=True)\n",
    "df_test = df_test[['pr-title', 'pr-article']]\n",
    "df_test['pr-article'] = df_test['pr-article'].astype('string')\n",
    "df_test['pr-title'] = df_test['pr-title'].astype('string')\n",
    "\n",
    "print('Valid dataframe size: ', len(df_valid))\n",
    "print('Test dataframe size:  ', len(df_test))"
   ],
   "id": "46ec09c086791090",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid dataframe size:  1431\n",
      "Test dataframe size:   1000\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:33.658190Z",
     "start_time": "2025-01-21T15:57:33.650278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_valid = df_valid[df_valid['pr-article'] != '']\n",
    "df_valid = df_valid[df_valid['pr-title'] != '']\n",
    "print('Valid dataframe size without blanks: ', len(df_valid))\n",
    "print('Test dataframe size without blanks:  ', len(df_test))"
   ],
   "id": "dc74b09dac19f369",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid dataframe size without blanks:  1405\n",
      "Test dataframe size without blanks:   1000\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning T5\n",
    "\n",
    "### Defininf model and loading tokenizer"
   ],
   "id": "3700b62f0c079c8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:37.671280Z",
     "start_time": "2025-01-21T15:57:33.690396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoProcessor\n",
    "import torch\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "484dd196ffd22276",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:38.883705Z",
     "start_time": "2025-01-21T15:57:37.688055Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoProcessor.from_pretrained(model_name)",
   "id": "433d1b38b46cea97",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocess data for model function",
   "id": "ca34ecef27d37bc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:38.904583Z",
     "start_time": "2025-01-21T15:57:38.900120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_article_length = 512\n",
    "max_title_length = 64\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    \"\"\"preprocess a batch of data for T5 title generation task\"\"\"\n",
    "    input_texts = ['summarize: ' + article for article in batch['pr-article']]\n",
    "    target_texts = batch['pr-title']\n",
    "\n",
    "    # tokenize input and target texts with padding to ensure consistent shape\n",
    "    tokenized_inputs = tokenizer(input_texts,\n",
    "                                  max_length=max_article_length,\n",
    "                                  truncation=True,\n",
    "                                 padding=True,\n",
    "                                  return_tensors=\"pt\")\n",
    "    tokenized_targets = tokenizer(target_texts,\n",
    "                               max_length=max_title_length,\n",
    "                               truncation=True,\n",
    "                                  padding=True,\n",
    "                               return_tensors=\"pt\")\n",
    "\n",
    "    # convert to dictionary compatible for Hugging Face Datasets\n",
    "    return {\n",
    "        \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "        \"labels\": tokenized_targets[\"input_ids\"]\n",
    "    }"
   ],
   "id": "e2996cf1cda9b648",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now apply preprocess function to all datasets:",
   "id": "32a05d1071686549"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:57:39.047553Z",
     "start_time": "2025-01-21T15:57:38.946879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_batch = df_train.sample(2).to_dict('list')  # Sample 2 rows from the training dataset\n",
    "print(\"Sample Batch:\\n\", sample_batch)\n",
    "preprocessed_batch = preprocess_data({\n",
    "    'pr-article': sample_batch['pr-article'],\n",
    "    'pr-title': sample_batch['pr-title']\n",
    "})\n",
    "\n",
    "# Print input IDs, attention masks, and labels\n",
    "print(\"Input IDs:\\n\", preprocessed_batch[\"input_ids\"])\n",
    "print(\"Attention Masks:\\n\", preprocessed_batch[\"attention_mask\"])\n",
    "print(\"Labels:\\n\", preprocessed_batch[\"labels\"])"
   ],
   "id": "a382a8403c1d81b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Batch:\n",
      " {'pr-title': ['Tiny Quantum Computer Solves Real Optimization Problem', 'Walk This Way: Novel Method Enables Infinite Walking in VR'], 'pr-article': ['Quantum computers have already managed to surpass ordinary computers in solving certain tasks - unfortunately, totally useless ones. The next milestone is to get them to do useful things. Researchers at Chalmers University of Technology, Sweden, have now shown that they can solve a small part of a real logistics problem with their small, but well-functioning quantum computer.\\nInterest in building quantum computers has gained considerable momentum in recent years, and feverish work is underway in many parts of the world. In 2019, Google\\'s research team made a major breakthrough when their quantum computer managed to solve a task far more quickly than the world\\'s best supercomputer. The downside is that the solved task had no practical use whatsoever - it was chosen because it was judged to be easy to solve for a quantum computer, yet very difficult for a conventional computer.\\nTherefore, an important task is now to find useful, relevant problems that are beyond the reach of ordinary computers, but which a relatively small quantum computer could solve.\\n\"We want to be sure that the quantum computer we are developing can help solve relevant problems early on. Therefore, we work in close collaboration with industrial companies,\" says theoretical physicist Giulia Ferrini, one of the leaders of Chalmers University of Technology\\'s quantum computer project, which began in 2018.\\nTogether with Goran Johansson, Giulia Ferrini led the theoretical work when a team of researchers at Chalmers, including an industrial doctoral student from the aviation logistics company Jeppesen, recently showed that a quantum computer can solve an instance of a real problem in the aviation industry.\\nThe algorithm proven on two qubits \\xad All airlines are faced with scheduling problems. For example, assigning individual aircraft to different routes represents an optimisation problem, one that grows very rapidly in size and complexity as the number of routes and aircraft increases.\\nResearchers hope that quantum computers will eventually be better at handling such problems than today\\'s computers. The basic building block of the quantum computer - the qubit - is based on completely different principles than the building blocks of today\\'s computers, allowing them to handle enormous amounts of information with relatively few qubits.\\nHowever, due to their different structure and function, quantum computers must be programmed in other ways than conventional computers. One proposed algorithm that is believed to be useful on early quantum computers is the so-called Quantum Approximate Optimization Algorithm (QAOA).\\nThe Chalmers research team has now successfully executed said algorithm on their quantum computer - a processor with two qubits - and they showed that it can successfully solve the problem of assigning aircraft to routes. In this first demonstration, the result could be easily verified as the scale was very small - it involved only two airplanes.\\nPotential to handle many aircraft With this feat, the researchers were first to show that the QAOA algorithm can solve the problem of assigning aircraft to routes in practice. They also managed to run the algorithm one level further than anyone before, an achievement that requires very good hardware and accurate control.\\n\"We have shown that we have the ability to map relevant problems onto our quantum processor. We still have a small number of qubits, but they work well. Our plan has been to first make everything work very well on a small scale, before scaling up,\" says Jonas Bylander, senior researcher responsible for the experimental design, and one of the leaders of the project of building a quantum computer at Chalmers.\\nThe theorists in the research team also simulated solving the same optimisation problem for up to 278 aircraft, which would require a quantum computer with 25 qubits.\\n\"The results remained good as we scaled up. This suggests that the QAOA algorithm has the potential to solve this type of problem at even larger scales,\" says Giulia Ferrini.\\nSurpassing today\\'s best computers would, however, require much larger devices. The researchers at Chalmers have now begun scaling up and are currently working with five quantum bits. The plan is to reach at least 20 qubits by 2021 while maintaining the high quality.\\nThe research results have been published in two articles in Physical Review Applied :\\nImproved Success Probability with Greater Circuit Depth for the Quantum Approximate Optimization Algorithm and\\nApplying the Quantum Approximate Optimization Algorithm to the Tail-Assignment Problem\\nMore about: The Swedish quest for a quantum computer The research is part of the Wallenberg Centre for Quantum Technology (WACQT), a twelve-year, billion-dollar investment with two main purposes: to develop Swedish expertise in quantum technology, and to build a useful quantum computer with at least one hundred quantum bits. The research centre is mainly funded by the Knut and Alice Wallenberg Foundation. Read more here:\\nEngineering of a Swedish quantum computer set to start (initial press release from 2017)\\nDiscover quantum technology (introduction to quantum technology)\\nQuantum computing (introduction to quantum computing)\\nWallenberg Centre for Quantum Technology (WACQT)\\nResearch in quantum computing and simulation (about quantum computing research within WACQT)\\nContact: Giulia Ferrini, Assistant Professor in Applied Quantum Physics, Chalmers University of Technology, ferrini@chalmers.se , +46-31-772 64 17 Jonas Bylander, Associate Professor in Quantum Technology, Chalmers University of Technology, jonas.bylander@chalmers.se , +46-31-772 51 32\\nChristian Borg Head of media relations\\n+46-31-772 3395\\nchristian.borg@chalmers.se\\n________________\\nChalmers University of Technology in Gothenburg, Sweden, conducts research and education in technology and natural sciences at a high international level. The university has 3100 employees and 10,000 students, and offers education in engineering, science, shipping and architecture.\\nWith scientific excellence as a basis, Chalmers promotes knowledge and technical solutions for a sustainable world. Through global commitment and entrepreneurship, we foster an innovative spirit, in close collaboration with wider society.The EU\\'s biggest research initiative - the Graphene Flagship - is coordinated by Chalmers. We are also leading the development of a Swedish quantum computer.\\nChalmers was founded in 1829 and has the same motto today as it did then: Avancez - forward.', 'In the ever-evolving landscape of virtual reality (VR) technology, a number of key hurdles remain. But a team of computer scientists have tackled one of the major challenges in VR that will greatly improve user experience--enabling an immersive virtual experience while being physically limited to one\\'s actual, real-world space. The research team will present their work at SIGGRAPH 2018.\\nComputer scientists from Stony Brook University, NVIDIA and Adobe have collaborated on a computational framework that gives VR users the perception of infinite walking in the virtual world--while limited to a small physical space. The framework also enables this free-walking experience for users without causing dizziness, shakiness, or discomfort typically tied to physical movement in VR. And, users avoid bumping into objects in the physical space while in the VR world.\\nTo do this, the researchers focused on manipulating a user\\'s walking direction by working with a basic natural phenomenon of the human eye, called saccade. Saccades are quick eye movements that occur when we look at a different point in our field of vision, like when scanning a room or viewing a painting. Saccades occur without our control and generally several times per second. During that time, our brains largely ignore visual input in a phenomenon known as \"saccadic suppression\"--leaving us completely oblivious to our temporary blindness, and the motion that our eyes performed.\\n\"In VR, we can display vast universes; however, the physical spaces in our homes and offices are much smaller,\" says lead author of the work, Qi Sun, a PhD student at Stony Brook University and former research intern at Adobe Research and NVIDIA. \"It\\'s the nature of the human eye to scan a scene by moving rapidly between points of fixation. We realized that if we rotate the virtual camera just slightly during saccades, we can redirect a user\\'s walking direction to simulate a larger walking space.\"\\nUsing a head- and eye-tracking VR headset, the researchers\\' new method detects saccadic suppression and redirects users during the resulting temporary blindness. When more redirection is required, researchers attempt to encourage saccades using a tailored version of subtle gaze direction--a method that can dynamically encourage saccades by creating points of contrast in our visual periphery.\\nThe team who authored the research, titled \"Towards Virtual Reality Infinite Walking: Dynamic Saccade Redirection,\" will present their work at SIGGRAPH 2018, held 12-16 August in Vancouver, British Columbia. The annual conference and exhibition showcases the world\\'s leading professionals, academics, and creative minds at the forefront of computer graphics and interactive techniques.\\nTo date, existing methods addressing infinite walking in VR have limited redirection capabilities or cause undesirable scene distortions; they have also been unable to avoid obstacles in the physical world, like desks and chairs. The team\\'s new method dynamically redirects the user away from these objects. The method runs fast, so it is able to avoid moving objects as well, such as other people in the same room.\\nThe researchers ran user studies and simulations to validate their new computational system, including having participants perform game-like search and retrieval tasks. Overall, virtual camera rotation was unnoticeable to users during episodes of saccadic suppression; they could not tell that they were being automatically redirected via camera manipulation. Additionally, in testing the team\\'s method for dynamic path planning in real-time, users were able to walk without running into walls and furniture, or moving objects like fellow VR users.\\n\"Currently in VR, it is still difficult to deliver a completely natural walking experience to VR users,\" says Sun. \"That is the primary motivation behind our work--to eliminate this constraint and enable fully immersive experiences in large virtual worlds.\"\\nThough mostly applicable to VR gaming, the new system could potentially be applied to other industries, including architectural design, education, and film production.\\n###\\nFor the full paper and video, visit the project page. Coauthors include Qi Sun (Stony Brook University, NVIDIA, Adobe), Anjul Patney (NVIDIA), Li-Yi Wei (Adobe), Omer Shapira (NVIDIA), Jingwan Lu (Adobe), Paul Asente (Adobe), Suwen Zhu (Stony Brook University), Morgan McGuire (NVIDIA), David Luebke (NVIDIA), and Arie Kaufman (Stony Brook University).\\nAbout ACM, ACM SIGGRAPH, and SIGGRAPH 2018 ACM, the Association for Computing Machinery, is the world\\'s largest educational and scientific computing society, uniting educators, researchers, and professionals to inspire dialogue, share resources, and address the field\\'s challenges. ACM SIGGRAPH is a special interest group within ACM that serves as an interdisciplinary community for members in research, technology, and applications in computer graphics and interactive techniques. SIGGRAPH is the world\\'s leading annual interdisciplinary educational experience showcasing the latest in computer graphics and interactive techniques. SIGGRAPH 2018, marking the 45th annual conference hosted by ACM SIGGRAPH, will take place from 12-16 August at the Vancouver Convention Centre in Vancouver, B.C. To register for SIGGRAPH 2018 and hear from the authors themselves, visit s2018.siggraph.org/attend/register .']}\n",
      "Input IDs:\n",
      " tensor([[21603,    10, 12716,  ...,  5058, 29044,     1],\n",
      "        [21603,    10,    86,  ...,     7,    57,     1]])\n",
      "Attention Masks:\n",
      " tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "Labels:\n",
      " tensor([[ 7138,    63, 12716,   440,  5491,   264,  8391,  2977, 29044,  5289,\n",
      "             1,     0,     0,     0,     0,     0,     0],\n",
      "        [10801,   100,  5994,    10, 24388,  7717,   695,   179,     7,    86,\n",
      "          5582,    17,    15, 19525,    16, 13893,     1]])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:15.585891Z",
     "start_time": "2025-01-21T15:57:39.072737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "tokenized_train_ds = Dataset.from_pandas(df_train).map(preprocess_data, batched=True, batch_size=batch_size)\n",
    "tokenized_valid_ds = Dataset.from_pandas(df_valid).map(preprocess_data, batched=True, batch_size=batch_size)\n",
    "tokenized_test_ds = Dataset.from_pandas(df_test).map(preprocess_data, batched=True, batch_size=batch_size)"
   ],
   "id": "b7503a1595a9529a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12735 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5b2831294db4cb785da85dda276084b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1405 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77ff9413e4664531a3cb8f48f3d10b73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d889016f49fa4c42996c53c8bee306e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:15.613926Z",
     "start_time": "2025-01-21T15:58:15.609703Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_train_ds.features",
   "id": "567e34181868da57",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pr-title': Value(dtype='string', id=None),\n",
       " 'pr-article': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare trainer",
   "id": "b4cf0810b03e87f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:15.885459Z",
     "start_time": "2025-01-21T15:58:15.653993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "model_folder = './T5-title-generator/'\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    # Training-specific configurations\n",
    "    num_train_epochs=1,  # Total number of training epochs\n",
    "    weight_decay=0.01,  # Apply L2 regularization to prevent overfitting\n",
    "    learning_rate=3e-5,  # Step size for the optimizer during training\n",
    "    optim=\"adamw_torch\",  # Optimizer,\n",
    "    warmup_steps=10,\n",
    "    predict_with_generate=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,  # Number of samples per eval batch for each device\n",
    "\n",
    "    # memory related settings\n",
    "    gradient_accumulation_steps=1,  # memory\n",
    "    gradient_checkpointing=True,  # memory\n",
    "    fp16 = False, # Speed\n",
    "    bf16=True,\n",
    "    tf32=False, # speed\n",
    "\n",
    "    # evaluation settings\n",
    "    output_dir=str(model_folder),  # Directory to save model checkpoints\n",
    "    eval_strategy=\"steps\",  # Evaluate model at specified step intervals\n",
    "    eval_steps=50,  # Perform evaluation every 100 training steps\n",
    "\n",
    "    # Checkpoint settings\n",
    "    save_strategy=\"steps\",  # Save model checkpoint at specified step intervals\n",
    "    save_steps=500,  # Save a model checkpoint every 500 training steps\n",
    "    load_best_model_at_end=True,  # Reload the best model at the end of training\n",
    "    save_total_limit=2,  # Retain only the best and the most recent model checkpoints\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # Experiment logging configurations\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50, # Log metrics each 100 steps\n",
    ")"
   ],
   "id": "eab151bec01b86c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data collator for the trainer:",
   "id": "46248450ecb1b3f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:15.920730Z",
     "start_time": "2025-01-21T15:58:15.906222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
    "                                       label_pad_token_id=-100)"
   ],
   "id": "d3b2cbd5fb58169c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Metric selection",
   "id": "19048e84d8fd9c41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:17.342057Z",
     "start_time": "2025-01-21T15:58:15.949206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from evaluate import load\n",
    "\n",
    "metric = load(\"rouge\")\n",
    "metric"
   ],
   "id": "c5bd5da779b048df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLsum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (f1),\n",
       "    rouge2: rouge_2 (f1),\n",
       "    rougeL: rouge_l (f1),\n",
       "    rougeLsum: rouge_lsum (f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = evaluate.load('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:17.523047Z",
     "start_time": "2025-01-21T15:58:17.366309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# function source: https://discuss.huggingface.co/t/trainer-warning-with-the-new-version/115446\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Clip predictions to ensure they are within valid token ID range\n",
    "    predictions = np.clip(predictions, 0, tokenizer.vocab_size - 1)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                     for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip()))\n",
    "                      for label in decoded_labels]\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "\n",
    "    # Extract ROUGE f1 scores (fix applied here)\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                       for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ],
   "id": "cefb07c20c1a92f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/neville/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:17.792483Z",
     "start_time": "2025-01-21T15:58:17.545279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example to test the `compute_metrics` function\n",
    "test_predictions = torch.tensor([[21603, 10, 2372, 3, 1, 0, 0, 0], [21603, 10, 94, 123, 1, 0, 0, 0]])\n",
    "test_labels = torch.tensor([[21603, 10, 2372, 3, -100, -100, -100, -100], [21603, 10, 94, 123, -100, -100, -100, -100]])\n",
    "\n",
    "# Call the function with example inputs\n",
    "example_metrics = compute_metrics((test_predictions, test_labels))\n",
    "\n",
    "# Print out the metrics results\n",
    "print(\"Example metrics:\\n\", example_metrics)"
   ],
   "id": "e70b60641421eb09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example metrics:\n",
      " {'rouge1': 100.0, 'rouge2': 100.0, 'rougeL': 100.0, 'rougeLsum': 100.0, 'gen_len': 5.0}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Start trainer",
   "id": "576f94ac6faffe50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:18.493866Z",
     "start_time": "2025-01-21T15:58:17.815396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ],
   "id": "f3f7a33890894f0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:18.524890Z",
     "start_time": "2025-01-21T15:58:18.520793Z"
    }
   },
   "cell_type": "code",
   "source": "print(model.config)",
   "id": "7233b359fe460d40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.48.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:18.630466Z",
     "start_time": "2025-01-21T15:58:18.589968Z"
    }
   },
   "cell_type": "code",
   "source": "print(model)",
   "id": "888b2a0cc3b146fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:18.740065Z",
     "start_time": "2025-01-21T15:58:18.658461Z"
    }
   },
   "cell_type": "code",
   "source": "model.num_parameters()",
   "id": "597526f38c21b107",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60506624"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model has 6x Encoders and 6x Decoders",
   "id": "4d7ff8db843d53a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T15:58:18.895312Z",
     "start_time": "2025-01-21T15:58:18.746778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_valid_ds,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "351ec7772a889e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113110/4080436188.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T16:45:21.899086Z",
     "start_time": "2025-01-21T15:58:18.944546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%time\n",
    "trainer.train()"
   ],
   "id": "ae52617556a91a49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 s, sys: 0 ns, total: 4 s\n",
      "Wall time: 9.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='544' max='1592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 544/1592 46:58 < 1:30:49, 0.19 it/s, Epoch 0.34/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.168100</td>\n",
       "      <td>4.621975</td>\n",
       "      <td>16.142000</td>\n",
       "      <td>4.043200</td>\n",
       "      <td>14.135500</td>\n",
       "      <td>14.286500</td>\n",
       "      <td>18.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2.962558</td>\n",
       "      <td>1.282800</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>1.065000</td>\n",
       "      <td>1.101700</td>\n",
       "      <td>1.555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.355300</td>\n",
       "      <td>2.636554</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>2.532000</td>\n",
       "      <td>2.577500</td>\n",
       "      <td>3.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.201900</td>\n",
       "      <td>2.490908</td>\n",
       "      <td>3.984200</td>\n",
       "      <td>0.964900</td>\n",
       "      <td>3.468700</td>\n",
       "      <td>3.533500</td>\n",
       "      <td>4.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.924800</td>\n",
       "      <td>2.403152</td>\n",
       "      <td>9.159100</td>\n",
       "      <td>2.205700</td>\n",
       "      <td>8.114300</td>\n",
       "      <td>8.186800</td>\n",
       "      <td>7.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.802800</td>\n",
       "      <td>2.357736</td>\n",
       "      <td>13.925800</td>\n",
       "      <td>3.533400</td>\n",
       "      <td>12.502900</td>\n",
       "      <td>12.560700</td>\n",
       "      <td>10.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>2.326510</td>\n",
       "      <td>17.362000</td>\n",
       "      <td>4.797700</td>\n",
       "      <td>15.795300</td>\n",
       "      <td>15.833000</td>\n",
       "      <td>12.645600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.599000</td>\n",
       "      <td>2.309196</td>\n",
       "      <td>19.912200</td>\n",
       "      <td>5.332700</td>\n",
       "      <td>18.091900</td>\n",
       "      <td>18.112400</td>\n",
       "      <td>13.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.699200</td>\n",
       "      <td>2.294737</td>\n",
       "      <td>20.136300</td>\n",
       "      <td>5.408200</td>\n",
       "      <td>18.259600</td>\n",
       "      <td>18.296100</td>\n",
       "      <td>13.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.624600</td>\n",
       "      <td>2.281069</td>\n",
       "      <td>21.208800</td>\n",
       "      <td>5.749700</td>\n",
       "      <td>19.061800</td>\n",
       "      <td>19.098500</td>\n",
       "      <td>14.620600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/transformers/trainer.py:2171\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2169\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2170\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2172\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2176\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/lib/python3.12/site-packages/transformers/trainer.py:2536\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2530\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m   2531\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs, num_items_in_batch)\n\u001B[1;32m   2533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2534\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2535\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m-> 2536\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misinf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss_step\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   2537\u001B[0m ):\n\u001B[1;32m   2538\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2539\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n\u001B[1;32m   2540\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Model training saved at 500 steps.\n",
    "\n",
    "**TODO**: Train some more steps"
   ],
   "id": "3d835c318e669106"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5f34d6fcb732dd6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
